<think>
The request is asking to:

1. Adjust and format the directory structure to make it more human-understandable
2. Create a detailed README with:
   - References to code
   - Walkthrough for each of the 3 tasks in the repo

Let's analyze what I can see from the provided code snippets to understand the repository structure and its tasks.

Based on the code I can see, this repository appears to contain code for time series analysis with three main tasks:

1. **Time Series Forecasting using DLinear** (dlinear_forecasting.py)
2. **Time Series Forecasting using DDPM** (ddpm_forecasting.py) 
3. **Time Series Imputation** (time_series_imputation.py)

There also seems to be some scripts for experiments and hyperparameter tuning:
- run_mps_experiments.py
- hyperparameter_tuning.py

Let me outline a better directory structure and create a comprehensive README:

```
/time-series-analysis/
├── models/
│   ├── dlinear.py         # DLinear model definition
│   ├── ddpm.py            # DDPM model definition
│   └── transformer.py     # Transformer model for imputation
├── tasks/
│   ├── forecasting/
│   │   ├── dlinear_forecasting.py
│   │   └── ddpm_forecasting.py
│   └── imputation/
│       └── time_series_imputation.py
├── utils/
│   ├── data_loader.py     # Common data loading utilities
│   └── metrics.py         # Evaluation metrics
├── experiments/
│   ├── run_all_tasks.py
│   ├── run_mps_experiments.py
│   └── hyperparameter_tuning.py
├── data/
│   └── exchange_rate.csv
├── results/               # Directory to save experiment results
└── README.md
```

Now I'll create a comprehensive README.md that explains the repository structure, tasks, and provides usage instructions.
</think>

# Improved Repository Structure and README

Let's reorganize the repository to make it more human-understandable and create a detailed README to guide users through the code.

## Directory Structure

First, let's create a better directory structure:

```
time-series-analysis/
├── models/
│   ├── dlinear.py         # DLinear model definition
│   ├── ddpm.py            # DDPM model definition
│   └── transformer.py     # Transformer model for imputation
├── tasks/
│   ├── forecasting/
│   │   ├── dlinear_forecasting.py
│   │   └── ddpm_forecasting.py
│   └── imputation/
│       └── time_series_imputation.py
├── utils/
│   ├── data_loader.py     # Common data loading utilities
│   └── metrics.py         # Evaluation metrics
├── experiments/
│   ├── run_all_tasks.py
│   ├── run_mps_experiments.py
│   └── hyperparameter_tuning.py
├── data/
│   └── exchange_rate.csv
├── results/               # Directory to save experiment results
└── README.md
```

Let's create the directories:

```bash
mkdir -p models tasks/forecasting tasks/imputation utils experiments data results
```

Now let's create a comprehensive README.md file:

```markdown
# Time Series Analysis

This repository contains implementations of various time series analysis techniques for forecasting and imputation tasks.

## Overview

Three main tasks are implemented:

1. **Time Series Forecasting with DLinear**: Linear model with decomposition for time series forecasting
2. **Time Series Forecasting with DDPM**: Diffusion model approach for probabilistic time series forecasting
3. **Time Series Imputation with Transformer**: Transformer-based architecture for missing value imputation

## Directory Structure

```
time-series-analysis/
├── models/                # Model definitions
├── tasks/                 # Task implementations
│   ├── forecasting/       # Forecasting tasks
│   └── imputation/        # Imputation tasks
├── utils/                 # Utility functions
├── experiments/           # Experiment runner scripts
├── data/                  # Data files
├── results/               # Saved experiment results
└── README.md              # This file
```

## Tasks

### 1. Time Series Forecasting with DLinear

DLinear is a simple yet effective model that decomposes time series data and applies linear layers for forecasting.

**Usage:**
```bash
python tasks/forecasting/dlinear_forecasting.py --seq_len 96 --pred_len 14 --batch_size 32 --epochs 100
```

**Parameters:**
- `seq_len`: Input sequence length (default: 96)
- `pred_len`: Prediction horizon (default: 14)
- `batch_size`: Batch size for training (default: 32)
- `epochs`: Number of training epochs (default: 100)
- `lr`: Learning rate (default: 0.001)
- `individual`: Whether to use individual models for each time series variable (default: False)

### 2. Time Series Forecasting with DDPM

DDPM (Denoising Diffusion Probabilistic Models) generate predictions by iteratively denoising random noise through a reverse diffusion process.

**Usage:**
```bash
python tasks/forecasting/ddpm_forecasting.py --seq_len 96 --pred_len 14 --batch_size 32 --epochs 100
```

**Parameters:**
- `seq_len`: Input sequence length (default: 96)
- `pred_len`: Prediction horizon (default: 14)
- `batch_size`: Batch size for training (default: 32)
- `epochs`: Number of training epochs (default: 100)
- `lr`: Learning rate (default: 0.001)

### 3. Time Series Imputation with Transformer

This task uses a Transformer encoder to reconstruct missing values in time series data.

**Usage:**
```bash
python tasks/imputation/time_series_imputation.py --mask_ratio 0.25 --seq_len 96 --batch_size 32 --epochs 100
```

**Parameters:**
- `mask_ratio`: Ratio of values to mask (default: 0.25)
- `seq_len`: Sequence length (default: 96)
- `batch_size`: Batch size (default: 32)
- `epochs`: Number of training epochs (default: 100)
- `lr`: Learning rate (default: 0.001)
- `d_model`: Transformer model dimension (default: 64)
- `nhead`: Number of attention heads (default: 4)
- `num_layers`: Number of transformer layers (default: 2)
- `dropout`: Dropout rate (default: 0.1)
- `patience`: Early stopping patience (default: 10)

## Running All Tasks

To run all tasks with default parameters:

```bash
python experiments/run_all_tasks.py
```

## MPS and GPU Support

The code supports various hardware acceleration:
- Apple Silicon GPUs via MPS (Metal Performance Shaders)
- NVIDIA GPUs via CUDA
- Defaults to CPU if neither is available

To run experiments with MPS support:
```bash
python experiments/run_mps_experiments.py
```

## Hyperparameter Tuning

For hyperparameter tuning using Weights & Biases:

```bash
python experiments/hyperparameter_tuning.py
```

## Data

The repository uses the Exchange Rate dataset, which includes daily exchange rates of 8 countries from 1990 to 2016.

## Results

All experiment results are saved in the `results/` directory:
- Model checkpoints
- Metrics JSON files
- Visualization images

## Requirements

- Python 3.8+
- PyTorch 1.12+
- NumPy
- Pandas
- Matplotlib
- scikit-learn
- Weights & Biases (for hyperparameter tuning)

## Installation

```bash
pip install torch numpy pandas matplotlib scikit-learn wandb
```
```

This README provides a comprehensive overview of your repository and guides users through your code and experiments.
